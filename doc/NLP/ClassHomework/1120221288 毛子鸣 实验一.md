# 实验一

## 1、采用的模型与方法

### 逻辑斯谛回归
逻辑斯谛回归通过一个线性函数来预测在输入的条件下，出现某一个类别的概率。在逻辑斯谛回归中，使用的是sigmoid函数，而在代码实现中，将其转化为softmax函数，最后输出的维度从单个值变为两个值，分别代表两类出现的概率。

具体公式为：
\[
P(y_1, y_2|\mathbf{x}) = Softmax(\mathbf{w}^T \mathbf{x} + b)
\]

同时优化模型使用的损失函数为交叉熵损失（Cross-Entropy Loss），其公式为：

\[
L(\mathbf{w}, b) = -\frac{1}{N} \sum_{i=1}^{N} \left[ y_i \log(p_i) + (1 - y_i) \log(1 - p_i) \right]
\]



在实验中，我们将输入的文本转化为输入向量，使用了sgns.target.word-word.dynwin5.thr10.neg5.dim300作为word embedding，将原输入文本转化为tokens，将tokens进行拼接、补全长度或截断长度后放入逻辑斯谛回归模型中，进行训练

### Naive Bayes算法

Naive Bayes算法的核心是贝叶斯公式，同时，进行了一个假设，对于某一个类别下，其中元素出现是彼此之间相互独立的，是一个非常强的假设，其计算公式为：

\[
P(C|X) = \frac{P(X|C) \cdot P(C)}{P(X)}
\]

在使用Naive Bayes算法进行情感识别时，首先使用jieba库对文本进行分词，之后使用参考材料中的停用词表对文本进行过滤，完成过滤后统计每个类别下词汇出现的概率，从而获得了公式中的$P(X|C)$，同时在先验设计的过程中，认为每类出现的概率相同，为0.5

在测试集中进行处理时，我们对所有的$P(C|X)$取对数进行相加，同时对于没出现的词汇，我们采用添加$log(\frac{1}{all-word-number})$进行处理

## 2、实验使用的数据、平台或工具
数据：Hotel分类数据（参考材料中），sgns.target.word-word.dynwin5.thr10.neg5.dim300词向量字典

平台：MacOS 15.0
conda: conda 24.5.0
python: Python 3.9.19
random seed: 42
工具(列举在requirements.txt中)：numpy，torch，jieba

## 3、实验过程(训练和测试)、实现过程描述
模型设计具体细节如第一部分所示。
### Naive Bayes
Naive Bayes训练过程中，训练集测试集的划分为8:2，将每句话的第一个字符设置为句子的类别（0或1）
测试过程中实现细节如第一部分所示。最后通过比较同一句话在两个类别下的对数概率和的大小，来判断句子所属的具体类型
### 逻辑斯谛回归
训练过程中，设置学习率为0.001，batch_size = 16，epoch为20，初始化使用torch默认的Kaming初始化处理。

## 4、实验结果

Naive Bayse准确率为 0.87
![naive bayse](1.png)
Logic Regression 准确率为 0.8425
![loss fig](2.png)
![training process](3.png)
